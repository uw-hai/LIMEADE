{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code implements a LIMEADE update using LIME and performs a simulated experiment using COCO segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, get_context, Process, set_start_method\n",
    "from skimage.segmentation import felzenszwalb, slic, quickshift\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.ndimage.measurements import find_objects\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from sklearn.utils import check_random_state\n",
    "import matplotlib.patches as patches\n",
    "from skimage.color import gray2rgb\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from collections import ChainMap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from random import choice\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import inspect\n",
    "import sklearn\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import types\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# import pycocotools COCO API\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# import some pytorch dependencies\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# handle pickle versioning issues\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "    \n",
    "# import img2vec for LIME pseudo-examples (need to install on device)\n",
    "from img2vec_pytorch import Img2Vec\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set random seed for reproducibility during development\n",
    "random.seed(28)\n",
    "np.random.seed(28)\n",
    "\n",
    "###################### set some hyperparameters ########################\n",
    "\n",
    "# define whether we are using ResNet 18 or ResNet 50:\n",
    "RESNET_50_BOOL = True\n",
    "\n",
    "# set +/- size for train and test sets\n",
    "TRAIN_SIZE = 1\n",
    "UPDATE_SIZE = 1 \n",
    "\n",
    "# set # of nearest neighbors\n",
    "N_NEAREST_NEIGHBORS = [1, 5, 10, 25, 50, 100]\n",
    "\n",
    "# sets weights of nearest neighbors (collectively)\n",
    "NEIGHBORS_WEIGHTS = [0.25, 0.5, 1, 2, 4]\n",
    "\n",
    "# sets number of samples for LIME to use\n",
    "N_LIME_SAMPLES = 500\n",
    "\n",
    "# sets desired class\n",
    "category_names = [\"bed\", \"carrot\", \"cow\", \"donut\", \"fire hydrant\", \"fork\", \"frisbee\", \"giraffe\", \"horse\", \"knife\", \"motorcycle\", \"potted plant\", \"scissors\", \"sink\", \"skateboard\", \"snowboard\", \"suitcase\", \"surfboard\", \"toothbrush\", \"baseball glove\"]\n",
    "\n",
    "# sets number of runs to do\n",
    "N_RUNS = 100\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed IDs\n",
    "if RESNET_50_BOOL:\n",
    "    embeddings_paths = sorted(glob.glob(\"../COCO/train2017_embeddings/*_full_embedding_50.npy\"))\n",
    "    preprocessed_ids = []\n",
    "    for i in range(0, len(embeddings_paths)):\n",
    "        img_id = int(embeddings_paths[i].split('/')[-1].split('_full_embedding_50.npy')[0])\n",
    "        preprocessed_ids.append(img_id)\n",
    "else:\n",
    "    embeddings_paths = sorted(glob.glob(\"../COCO/train2017_embeddings/*_full_embedding_18.npy\"))\n",
    "    preprocessed_ids = []\n",
    "    for i in range(0, len(embeddings_paths)):\n",
    "        img_id = int(embeddings_paths[i].split('/')[-1].split('_full_embedding_18.npy')[0])\n",
    "        preprocessed_ids.append(img_id)\n",
    "        \n",
    "        \n",
    "### Now, we load in all embeddings (this step takes a while and only needs to be run once): ####\n",
    "# load pre-processed IDs\n",
    "if RESNET_50_BOOL:\n",
    "    all_embeddings_paths = sorted(glob.glob(\"../COCO/train2017_embeddings/*50.npy\"))\n",
    "else:\n",
    "    all_embeddings_paths = sorted(glob.glob(\"../COCO/train2017_embeddings/*18.npy\"))\n",
    "        \n",
    " \n",
    "# Handles whether to preprocess the embeddings and paths (default to not, as this is time-consuming; only re-run\n",
    "# if new images added to unlabeled pool)\n",
    "REDOWNLOAD = True\n",
    "\n",
    "preprocessed_embeddings_paths = [] \n",
    "preprocessed_embeddings = []\n",
    "\n",
    "if REDOWNLOAD: \n",
    "\n",
    "    for i in tqdm(range(0, len(preprocessed_ids))):\n",
    "        index = preprocessed_ids[i]\n",
    "        for j in range(0, len(all_embeddings_paths)):\n",
    "            if \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(index) in all_embeddings_paths[j]:\n",
    "                preprocessed_embeddings_paths.append(all_embeddings_paths[j])\n",
    "                # NOTE: CANNOT BREAK HERE!!!\n",
    "                \n",
    "    f = open(\"preprocessed_embeddings_paths.pkl\", \"wb\")\n",
    "    pickle.dump(preprocessed_embeddings_paths, f)\n",
    "    \n",
    "    for i in tqdm(range(0, len(preprocessed_embeddings_paths))):\n",
    "        preprocessed_embeddings.append(np.load(preprocessed_embeddings_paths[i]))\n",
    "    preprocessed_embeddings = np.array(preprocessed_embeddings)\n",
    "    np.save(\"preprocessed_embeddings.npy\", preprocessed_embeddings)\n",
    "\n",
    "    \n",
    "# saves having to re-run\n",
    "else:\n",
    "    preprocessed_embeddings_paths = pickle.load(open(\"preprocessed_embeddings_paths.pkl\", \"rb\"))\n",
    "    preprocessed_embeddings = np.load(\"preprocessed_embeddings.npy\")\n",
    "    \n",
    "print(\"DONE LOADING EMBEDDINGS\")\n",
    "################################################################################################\n",
    "\n",
    "### Next, we handle COCO category filtering & the function for mapping COCO segments to superpixels: ###\n",
    "\n",
    "dataDir='..'\n",
    "dataType='train2017'\n",
    "annFile='../COCO/annotations/instances_' + dataType + '.json'\n",
    "\n",
    "# initialize COCO api for instance annotations\n",
    "coco=COCO(annFile)\n",
    "\n",
    "# display COCO categories and supercategories\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "\n",
    "# function for retrieving relevant superpixels for an image, given the image ID and category ID\n",
    "def retrieve_relevant_superpixels(imgId, catId):\n",
    "    \n",
    "    # first, load relevant annotations\n",
    "    annIds = coco.getAnnIds(catIds=[catId], imgIds=[imgId])\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    \n",
    "    # we load the superpixels for image\n",
    "    segments_filename = \"../COCO/train2017_segments/\" + '{:>012d}'.format(imgId) + \".npy\"\n",
    "\n",
    "    # load superpixels\n",
    "    segments = np.load(segments_filename)\n",
    "    \n",
    "    # load image\n",
    "    img = coco.loadImgs([imgId])[0]\n",
    "        \n",
    "    # create image mask for segments\n",
    "    mask = np.zeros((img['height'],img['width']))\n",
    "        \n",
    "    # add segments\n",
    "    for ann in anns:\n",
    "        \n",
    "        # convert segments to pixel mask\n",
    "        mask = np.maximum(coco.annToMask(ann), mask)\n",
    "\n",
    "    # convert to correct dimensions (224 x 224) for comparing against segments\n",
    "    transformed_mask = np.array(pill_transf(Image.fromarray(np.uint8(mask)*255)))\n",
    "\n",
    "    # next, we find the ids that overlap with segments\n",
    "    relevant_superpixels = np.unique(segments[transformed_mask != 0])\n",
    "\n",
    "    return(relevant_superpixels)\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "def process_category(name):\n",
    "        \n",
    "    # get all images containing given category\n",
    "    category = name\n",
    "    catIds = coco.getCatIds(catNms=[category])\n",
    "    imgIds = coco.getImgIds(catIds=catIds)\n",
    "    \n",
    "    # filter to find ones in preprocessed\n",
    "    filtered_img_ids = []\n",
    "    for i in range(0, len(imgIds)):\n",
    "        if imgIds[i] in preprocessed_ids:\n",
    "            filtered_img_ids.append(imgIds[i])\n",
    "    \n",
    "    # shuffle filtered IDs to get a new classifier each time\n",
    "    np.random.shuffle(filtered_img_ids)\n",
    "    \n",
    "    # sets positive train ids to those in the selected class\n",
    "    positive_train_ids = filtered_img_ids[:TRAIN_SIZE]\n",
    "    print(positive_train_ids)\n",
    "    print(filtered_img_ids[:5])\n",
    "    \n",
    "    # sets positive val ids to those in the selected class\n",
    "    positive_update_ids = filtered_img_ids[TRAIN_SIZE:TRAIN_SIZE+UPDATE_SIZE]\n",
    "    \n",
    "    # we now randomly assign negative train ids\n",
    "    negative_train_ids = []\n",
    "    \n",
    "    # need to copy or else append to positive_train_ids as well\n",
    "    already_used = positive_train_ids.copy()\n",
    "    \n",
    "    for i in range(0, TRAIN_SIZE):\n",
    "        random_negative = choice(preprocessed_ids)\n",
    "        while (random_negative in already_used) or (random_negative in filtered_img_ids):\n",
    "            random_negative = choice(preprocessed_ids)\n",
    "        negative_train_ids.append(random_negative)\n",
    "        already_used.append(random_negative)\n",
    "        \n",
    "    negative_update_ids = []\n",
    "    \n",
    "    for i in range(0, UPDATE_SIZE):\n",
    "        random_negative = choice(preprocessed_ids)\n",
    "        while (random_negative in already_used) or (random_negative in filtered_img_ids):\n",
    "            random_negative = choice(preprocessed_ids)\n",
    "        negative_update_ids.append(random_negative)\n",
    "        already_used.append(random_negative)\n",
    "    \n",
    "    \n",
    "    print(\"TRAIN IDS: \")\n",
    "    print(positive_train_ids)\n",
    "    print(negative_train_ids)\n",
    "    print(positive_update_ids)\n",
    "    print(negative_update_ids)\n",
    "    \n",
    "                \n",
    "    used_ids = []\n",
    "    used_ids += positive_train_ids.copy()\n",
    "    used_ids += positive_update_ids.copy()\n",
    "    used_ids += negative_train_ids.copy()\n",
    "    used_ids += negative_update_ids.copy()\n",
    "        \n",
    "    unlabeled_pool = [x for x in preprocessed_ids if x not in used_ids]\n",
    "    \n",
    "    return [positive_train_ids, negative_train_ids, positive_update_ids, negative_update_ids, unlabeled_pool, filtered_img_ids]\n",
    "\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified LIME code:\n",
    "\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "from skimage.color import gray2rgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from lime import lime_base\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "\n",
    "\n",
    "class ModifiedImageExplanation(object):\n",
    "    def __init__(self, image, segments):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            image: 3d numpy array\n",
    "            segments: 2d numpy array, with the output from skimage.segmentation\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.segments = segments\n",
    "        self.intercept = {}\n",
    "        self.local_exp = {}\n",
    "        self.local_pred = {}\n",
    "        self.score = {}\n",
    "\n",
    "    def get_image_and_mask(self, label, positive_only=True, negative_only=False, hide_rest=False,\n",
    "                           num_features=5, min_weight=0.):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            label: label to explain\n",
    "            positive_only: if True, only take superpixels that positively contribute to\n",
    "                the prediction of the label.\n",
    "            negative_only: if True, only take superpixels that negatively contribute to\n",
    "                the prediction of the label. If false, and so is positive_only, then both\n",
    "                negativey and positively contributions will be taken.\n",
    "                Both can't be True at the same time\n",
    "            hide_rest: if True, make the non-explanation part of the return\n",
    "                image gray\n",
    "            num_features: number of superpixels to include in explanation\n",
    "            min_weight: minimum weight of the superpixels to include in explanation\n",
    "        Returns:\n",
    "            (image, mask), where image is a 3d numpy array and mask is a 2d\n",
    "            numpy array that can be used with\n",
    "            skimage.segmentation.mark_boundaries\n",
    "        \"\"\"\n",
    "        if label not in self.local_exp:\n",
    "            raise KeyError('Label not in explanation')\n",
    "        if positive_only & negative_only:\n",
    "            raise ValueError(\"Positive_only and negative_only cannot be true at the same time.\")\n",
    "        segments = self.segments\n",
    "        image = self.image\n",
    "        exp = self.local_exp[label]\n",
    "        mask = np.zeros(segments.shape, segments.dtype)\n",
    "        if hide_rest:\n",
    "            temp = np.zeros(self.image.shape)\n",
    "        else:\n",
    "            temp = self.image.copy()\n",
    "        if positive_only:\n",
    "            fs = [x[0] for x in exp\n",
    "                  if x[1] > 0 and x[1] > min_weight][:num_features]\n",
    "        if negative_only:\n",
    "            fs = [x[0] for x in exp\n",
    "                  if x[1] < 0 and abs(x[1]) > min_weight][:num_features]\n",
    "        if positive_only or negative_only:\n",
    "            for f in fs:\n",
    "                temp[segments == f] = image[segments == f].copy()\n",
    "                mask[segments == f] = 1\n",
    "            return temp, mask\n",
    "        else:\n",
    "            for f, w in exp[:num_features]:\n",
    "                if np.abs(w) < min_weight:\n",
    "                    continue\n",
    "                c = 0 if w < 0 else 1\n",
    "                mask[segments == f] = -1 if w < 0 else 1\n",
    "                temp[segments == f] = image[segments == f].copy()\n",
    "                temp[segments == f, c] = np.max(image)\n",
    "            return temp, mask\n",
    "\n",
    "class ModifiedLimeImageExplainer(object):\n",
    "    \"\"\"Explains predictions on Image (i.e. matrix) data.\n",
    "    For numerical features, perturb them by sampling from a Normal(0,1) and\n",
    "    doing the inverse operation of mean-centering and scaling, according to the\n",
    "    means and stds in the training data. For categorical features, perturb by\n",
    "    sampling according to the training distribution, and making a binary\n",
    "    feature that is 1 when the value is the same as the instance being\n",
    "    explained.\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_width=.25, kernel=None, verbose=False,\n",
    "                 feature_selection='auto', random_state=None):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "            If None, defaults to sqrt(number of columns) * 0.75.\n",
    "            kernel: similarity kernel that takes euclidean distances and kernel\n",
    "                width as input and outputs weights in (0,1). If None, defaults to\n",
    "                an exponential kernel.\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "        \"\"\"\n",
    "        kernel_width = float(kernel_width)\n",
    "\n",
    "        if kernel is None:\n",
    "            def kernel(d, kernel_width):\n",
    "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
    "\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.feature_selection = feature_selection\n",
    "        self.base = lime_base.LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
    "\n",
    "    def explain_instance(self, image, embedding, classifier_fn, labels=(1,),\n",
    "                         hide_color=None,\n",
    "                         top_labels=5, num_features=100000, num_samples=1000,\n",
    "                         batch_size=4,\n",
    "                         segmentation_fn=None,\n",
    "                         distance_metric='cosine',\n",
    "                         model_regressor=None,\n",
    "                         random_seed=None,\n",
    "                         progress_bar=True):\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "        First, we generate neighborhood data by randomly perturbing features\n",
    "        from the instance (see __data_inverse). We then learn locally weighted\n",
    "        linear models on this neighborhood data to explain each of the classes\n",
    "        in an interpretable way (see lime_base.py).\n",
    "        Args:\n",
    "            image: 3 dimension RGB image. If this is only two dimensional,\n",
    "                we will assume it's a grayscale image and call gray2rgb.\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a numpy array and outputs prediction probabilities.  For\n",
    "                ScikitClassifiers , this is classifier.predict_proba.\n",
    "            labels: iterable with labels to be explained.\n",
    "            hide_color: TODO\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            batch_size: TODO\n",
    "            distance_metric: the distance metric to use for weights.\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "            to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
    "            and 'sample_weight' as a parameter to model_regressor.fit()\n",
    "            segmentation_fn: SegmentationAlgorithm, wrapped skimage\n",
    "            segmentation function\n",
    "            random_seed: integer used as random seed for the segmentation\n",
    "                algorithm. If None, a random integer, between 0 and 1000,\n",
    "                will be generated using the internal random number generator.\n",
    "            progress_bar: if True, show tqdm progress bar.\n",
    "        Returns:\n",
    "            An ImageExplanation object (see lime_image.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 2:\n",
    "            image = gray2rgb(image)\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_state.randint(0, high=1000)\n",
    "\n",
    "        if segmentation_fn is None:\n",
    "            segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=4,\n",
    "                                                    max_dist=200, ratio=0.2,\n",
    "                                                    random_seed=random_seed)\n",
    "        try:\n",
    "            segments = segmentation_fn(image)\n",
    "        except ValueError as e:\n",
    "            raise e\n",
    "\n",
    "        fudged_image = image.copy()\n",
    "        if hide_color is None:\n",
    "            for x in np.unique(segments):\n",
    "                fudged_image[segments == x] = (\n",
    "                    np.mean(image[segments == x][:, 0]),\n",
    "                    np.mean(image[segments == x][:, 1]),\n",
    "                    np.mean(image[segments == x][:, 2]))\n",
    "        else:\n",
    "            fudged_image[:] = hide_color\n",
    "\n",
    "        top = labels\n",
    "\n",
    "        data, labels = self.data_labels(image, fudged_image, segments,\n",
    "                                        classifier_fn, num_samples,\n",
    "                                        batch_size=batch_size,\n",
    "                                        progress_bar=progress_bar)\n",
    "\n",
    "        distances = sklearn.metrics.pairwise_distances(\n",
    "            data,\n",
    "            data[0].reshape(1, -1),\n",
    "            metric=distance_metric\n",
    "        ).ravel()\n",
    "\n",
    "        ret_exp = ModifiedImageExplanation(image, segments)\n",
    "        if top_labels:\n",
    "            top = np.argsort(labels[0])[-top_labels:]\n",
    "            ret_exp.top_labels = list(top)\n",
    "            ret_exp.top_labels.reverse()\n",
    "        for label in top:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score[label],\n",
    "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
    "                data, labels, distances, label, num_features,\n",
    "                model_regressor=model_regressor,\n",
    "                feature_selection=self.feature_selection)\n",
    "        return ret_exp\n",
    "\n",
    "    def data_labels(self,\n",
    "                    image,\n",
    "                    fudged_image,\n",
    "                    segments,\n",
    "                    classifier_fn,\n",
    "                    num_samples,\n",
    "                    batch_size=10, # altered for this version\n",
    "                    progress_bar=True):\n",
    "        \"\"\"Generates images and predictions in the neighborhood of this image.\n",
    "        Args:\n",
    "            image: 3d numpy array, the image\n",
    "            fudged_image: 3d numpy array, image to replace original image when\n",
    "                superpixel is turned off\n",
    "            segments: segmentation of the image\n",
    "            classifier_fn: function that takes a list of images and returns a\n",
    "                matrix of prediction probabilities\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            batch_size: classifier_fn will be called on batches of this size.\n",
    "            progress_bar: if True, show tqdm progress bar.\n",
    "        Returns:\n",
    "            A tuple (data, labels), where:\n",
    "                data: dense num_samples * num_superpixels\n",
    "                labels: prediction probabilities matrix\n",
    "        \"\"\"\n",
    "        n_features = np.unique(segments).shape[0]\n",
    "        data = self.random_state.randint(0, 2, num_samples * n_features)\\\n",
    "            .reshape((num_samples, n_features))\n",
    "        labels = []\n",
    "        data[0, :] = 1\n",
    "        \n",
    "        ######################## HACK HERE FOR EMBEDDINGS ####################\n",
    "        imgs = []\n",
    "        pil_imgs = []\n",
    "        embeddings = []\n",
    "        \n",
    "        # load in img2vec\n",
    "        # we choose resnet embeddings\n",
    "        img2vec_resnet_50 = Img2Vec(cuda=True, model='resnet-50') \n",
    "        img2vec_resnet_18 = Img2Vec(cuda=True, model='resnet-18') \n",
    "        \n",
    "        rows = tqdm(data) if progress_bar else data\n",
    "        for row in rows:\n",
    "            temp = copy.deepcopy(image)\n",
    "            zeros = np.where(row == 0)[0]\n",
    "            mask = np.zeros(segments.shape).astype(bool)\n",
    "            for z in zeros:\n",
    "                mask[segments == z] = True\n",
    "            temp[mask] = fudged_image[mask]\n",
    "            imgs.append(temp)\n",
    "                                \n",
    "            for img in imgs:\n",
    "                pil_img = Image.fromarray(np.uint8(img))\n",
    "                pil_imgs.append(pil_img)\n",
    "\n",
    "            # generate embedding using img2vec\n",
    "            if RESNET_50_BOOL:\n",
    "                batch_embeddings_resnet = img2vec_resnet_50.get_vec(pil_imgs, tensor=False)\n",
    "            else:\n",
    "                batch_embeddings_resnet = img2vec_resnet_18.get_vec(pil_imgs, tensor=False)\n",
    "            \n",
    "            # update the embeddings array\n",
    "            if len(embeddings) == 0:\n",
    "                embeddings = batch_embeddings_resnet\n",
    "            else:\n",
    "                embeddings = np.concatenate((embeddings, batch_embeddings_resnet), axis=0)\n",
    "\n",
    "            if len(embeddings) == batch_size:\n",
    "                preds = classifier_fn(np.array(embeddings))\n",
    "                labels.extend(preds)\n",
    "                imgs = []\n",
    "                pil_imgs = [] # need to reset the PIL versions of images and embeddings here, too\n",
    "                embeddings = [] \n",
    "                \n",
    "        if len(embeddings) > 0:\n",
    "            preds = classifier_fn(np.array(embeddings))\n",
    "            labels.extend(preds)\n",
    "            \n",
    "        ###################################################################\n",
    "\n",
    "        return data, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_name in category_names:\n",
    "    \n",
    "    print(\"STARTING CATEGORY: \" + category_name)\n",
    "    \n",
    "    for run_index in range(0, N_RUNS):\n",
    "\n",
    "        print(\"STARTING RUN \" + str(run_index) + \"...\")\n",
    "\n",
    "        ########### set a partition here ############\n",
    "        partition = process_category(category_name)\n",
    "\n",
    "        positive_train_ids = partition[0]\n",
    "        negative_train_ids = partition[1]\n",
    "        positive_update_ids = partition[2]\n",
    "        negative_update_ids = partition[3]\n",
    "        unlabeled_pool = partition[4] # stores indices of preprocessed but unlabeled images to draw from during update\n",
    "        filtered_img_ids = partition[5] # need to remember all of the positive examples, too, to determine whether a prediction is correct\n",
    "\n",
    "        intersections = np.intersect1d(filtered_img_ids, unlabeled_pool)\n",
    "\n",
    "        ############################################\n",
    "\n",
    "        # Next, we load in the relevant embeddings based on the unlabeled pool: #\n",
    "        preprocessed_ids = np.array(preprocessed_ids, dtype=int)\n",
    "\n",
    "        relevant_embeddings = []\n",
    "        relevant_embeddings_paths = []\n",
    "\n",
    "        for i in range(0, len(preprocessed_embeddings_paths)):\n",
    "            path = preprocessed_embeddings_paths[i]\n",
    "            path_index = int(path[29:41])\n",
    "\n",
    "            if path_index in positive_train_ids:\n",
    "                continue\n",
    "            if path_index in negative_train_ids:\n",
    "                continue\n",
    "            if path_index in positive_update_ids:\n",
    "                continue\n",
    "            if path_index in negative_update_ids:\n",
    "                continue\n",
    "                \n",
    "            relevant_embeddings.append(preprocessed_embeddings[i])\n",
    "            relevant_embeddings_paths.append(preprocessed_embeddings_paths[i]) \n",
    "\n",
    "        relevant_embeddings = np.array(relevant_embeddings)\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "        ### Next, we train a linear model on the embeddings using the labels above and evaluate performance:\n",
    "        positive_train_embeddings = []\n",
    "        for i in range(0, len(positive_train_ids)):\n",
    "            if RESNET_50_BOOL:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(positive_train_ids[i]) + \"_full_embedding_50.npy\"\n",
    "            else:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(positive_train_ids[i]) + \"_full_embedding_18.npy\"\n",
    "            embedding = np.load(filename)\n",
    "            positive_train_embeddings.append(embedding)\n",
    "        positive_train_embeddings = np.array(positive_train_embeddings)\n",
    "\n",
    "        negative_train_embeddings = []\n",
    "        for i in range(0, len(negative_train_ids)):\n",
    "            if RESNET_50_BOOL:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(negative_train_ids[i]) + \"_full_embedding_50.npy\"\n",
    "            else:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(negative_train_ids[i]) + \"_full_embedding_18.npy\"\n",
    "            embedding = np.load(filename)\n",
    "            negative_train_embeddings.append(embedding)\n",
    "        negative_train_embeddings = np.array(negative_train_embeddings)\n",
    "\n",
    "        # sets training data\n",
    "        train_X = np.concatenate((positive_train_embeddings, negative_train_embeddings), axis=0)\n",
    "        train_y = np.concatenate((np.ones(len(positive_train_embeddings)), np.zeros(len(negative_train_embeddings))))\n",
    "        train_sample_weight = np.ones(len(positive_train_embeddings) + len(negative_train_embeddings))\n",
    "\n",
    "        # sets val data from precomputed partitions\n",
    "        val_X = np.load(\"splits/\" + category_name + \"_val_X.npy\")\n",
    "        val_y = np.load(\"splits/\" + category_name + \"_val_y.npy\")\n",
    "        \n",
    "        # sets test data from precomputed partitions\n",
    "        test_X = np.load(\"splits/\" + category_name + \"_test_X.npy\")\n",
    "        test_y = np.load(\"splits/\" + category_name + \"_test_y.npy\")\n",
    "        \n",
    "        # instantiate linear model and train\n",
    "        clf = LogisticRegression(random_state=0)\n",
    "        clf.fit(train_X, train_y)\n",
    "\n",
    "        # evaluate linear model's performance\n",
    "        train_score = clf.score(train_X, train_y)\n",
    "        original_val_score = clf.score(val_X, val_y)\n",
    "        original_test_score = clf.score(test_X, test_y)\n",
    "        \n",
    "        print(\"SCORES: \")\n",
    "        print(train_score, original_val_score, original_test_score)\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        # Next, we randomly draw a NEGATIVE from the validation data to inspect and understand why the prediction has been made. \n",
    "        def get_image(index):\n",
    "            filename = \"../COCO/train2017/\" + '{:>012d}'.format(index) + \".jpg\"\n",
    "            with open(os.path.abspath(filename), 'rb') as f:\n",
    "                with Image.open(f) as img:\n",
    "                    return img.convert('RGB') \n",
    "\n",
    "        def get_embedding(index):\n",
    "            if RESNET_50_BOOL:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(index) + \"_full_embedding_50.npy\"\n",
    "            else:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(index) + \"_full_embedding_18.npy\"\n",
    "            embedding = np.load(filename)\n",
    "            return embedding\n",
    "\n",
    "        # first, let's randomly grab a negative image from VAL\n",
    "        negative_instance_index = random.choice(negative_update_ids)\n",
    "\n",
    "        negative_img = get_image(negative_instance_index)\n",
    "        negative_instance_embedding = get_embedding(negative_instance_index)\n",
    "\n",
    "        prediction = clf.predict(np.array([negative_instance_embedding]))\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        ### Next, we define the image transform needed and then run the explain instance functionality from the modified LIME code:\n",
    "        # need this transform defined in https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb\n",
    "        def get_pil_transform(): \n",
    "            transf = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.CenterCrop(224)\n",
    "            ])    \n",
    "\n",
    "            return transf \n",
    "\n",
    "        pill_transf = get_pil_transform()\n",
    "\n",
    "        from lime import lime_image\n",
    "\n",
    "        explainer = ModifiedLimeImageExplainer()\n",
    "        explanation = explainer.explain_instance(np.array(pill_transf(negative_img)),\n",
    "                                                 negative_instance_embedding, # pass in embedding here\n",
    "                                                 clf.predict_proba, # classification function\n",
    "                                                 top_labels=5, \n",
    "                                                 hide_color=0, \n",
    "                                                 batch_size=1,\n",
    "                                                 num_samples=N_LIME_SAMPLES) # number of images that will be sent to classification function\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "        ### Next, we show the explanation and retrieve the corresponding crop from the preprocessed data:\n",
    "        # retrieve explanation\n",
    "        temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=1, hide_rest=False)\n",
    "\n",
    "        # we now find the segment mask for the explanation\n",
    "        exp_segment_rows, exp_segment_cols = np.where(mask != 0)\n",
    "\n",
    "        # now, we load the mask from the corresponding image to map back to the crop\n",
    "        segments_filename = \"../COCO/train2017_segments/\" + '{:>012d}'.format(negative_instance_index) + \".npy\"\n",
    "        segments = np.load(segments_filename)\n",
    "\n",
    "        # for each pixel in the explanation mask, we retrieve the ID in the mask and check to see what it corresponds to\n",
    "        segment_ids = []\n",
    "        for i in range(0, len(exp_segment_rows)):\n",
    "            segment_ids.append(segments[exp_segment_rows[i], exp_segment_cols[i]])\n",
    "\n",
    "        # we now check to make sure that this is all one segment\n",
    "        try:\n",
    "            assert len(set(segment_ids)) == 1\n",
    "        except:\n",
    "            print(\"Explanation weirdness - multiple superpixels reported with top explanation ID - maybe strange tie?\")\n",
    "\n",
    "        # if so, we record the segment ID\n",
    "        negative_explanation_segment_id = segment_ids[0]\n",
    "\n",
    "        # we now retrieve the cropped image\n",
    "        crop_filename = \"../COCO/train2017_crops/\" + '{:>012d}'.format(negative_instance_index) + \"_\" + str(negative_explanation_segment_id) + \".jpg\"\n",
    "\n",
    "        # save most-weighted superpixel in LIME explanation as superpixel to act on\n",
    "        negative_action_segment_ids = [negative_explanation_segment_id]\n",
    "\n",
    "        # thumbs-down this\n",
    "        negative_feedback = 0\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        ### Next, we show the superpixel acted on, along with the annotation, and set the embedding:\n",
    "\n",
    "        mask = np.zeros(segments.shape)\n",
    "        for negative_action_segment_id in negative_action_segment_ids:\n",
    "            mask += (segments == negative_action_segment_id)\n",
    "\n",
    "        # now, we generate bounding box around these segments\n",
    "        boxes = find_objects(mask.astype(int))\n",
    "        try:\n",
    "            box = boxes[0]\n",
    "        except:\n",
    "            continue\n",
    "        w1 = box[1].start\n",
    "        w2 = box[1].stop\n",
    "        h1 = box[0].start\n",
    "        h2 = box[0].stop\n",
    "        pil_im = Image.fromarray(np.uint8(np.array(pill_transf(get_image(negative_instance_index))))).convert('RGB')\n",
    "        cropped = pil_im.crop((w1, h1, w2, h2))\n",
    "\n",
    "\n",
    "        # we now feed it to the embedding model\n",
    "        # load in img2vec\n",
    "        # generate appropriate ResNet embedding\n",
    "        if RESNET_50_BOOL:\n",
    "            img2vec_resnet_50 = Img2Vec(cuda=True, model='resnet-50') \n",
    "            crop_embedding = img2vec_resnet_50.get_vec(cropped, tensor=False)\n",
    "        else:\n",
    "            img2vec_resnet_18 = Img2Vec(cuda=True, model='resnet-18') \n",
    "            crop_embedding = img2vec_resnet_18.get_vec(cropped, tensor=False)\n",
    "\n",
    "        negative_superpixel_embedding = crop_embedding\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        ### Next, we retrieve the most similar crops across all images:\n",
    "\n",
    "        max_neighbors = np.max(np.array(N_NEAREST_NEIGHBORS))\n",
    "\n",
    "        # next, we compute the nearest neighbors using the embeddings \n",
    "        cosine_vectors = relevant_embeddings - crop_embedding\n",
    "        cosine_distances = np.sum(np.abs(cosine_vectors)**2,axis=-1) #**(1./2)\n",
    "        negative_nearest_indices = np.argsort(cosine_distances)[:np.max(np.array(N_NEAREST_NEIGHBORS))]  \n",
    "        \n",
    "        negative_instance_cosine_vectors = relevant_embeddings - negative_instance_embedding\n",
    "        negative_instance_cosine_distances = np.sum(np.abs(negative_instance_cosine_vectors)**2,axis=-1)\n",
    "        negative_instance_nearest_indices = np.argsort(negative_instance_cosine_distances)[:np.max(np.array(N_NEAREST_NEIGHBORS))*1000]\n",
    "        negative_instance_nearest_embeddings = []\n",
    "\n",
    "        for i in range(0, len(negative_instance_nearest_indices)):\n",
    "            # we want the full images here!\n",
    "            if \"_full\" in relevant_embeddings_paths[negative_instance_nearest_indices[i]]:\n",
    "                negative_instance_nearest_embeddings.append(np.load(relevant_embeddings_paths[negative_instance_nearest_indices[i]]))\n",
    "                coco_id = int(relevant_embeddings_paths[negative_instance_nearest_indices[i]].split('_')[1].split(\"embeddings/\")[1])\n",
    "                if len(negative_instance_nearest_embeddings) == np.max(np.array(N_NEAREST_NEIGHBORS)):\n",
    "                    break\n",
    "        negative_instance_nearest_embeddings = np.array(negative_instance_nearest_embeddings)\n",
    "\n",
    "        nn_full_image_embeddings = []\n",
    "        nn_superpixel_embeddings = []\n",
    "\n",
    "        # we now show the nearest crops and grab the relevant full image embeddings\n",
    "        for i in range(0, max_neighbors):\n",
    "\n",
    "            index = negative_nearest_indices[i] # if scikit-learn, use [0][i] here\n",
    "            if RESNET_50_BOOL:\n",
    "                nn_crop_filename = relevant_embeddings_paths[index].replace(\"_embedding_50.npy\", \".jpg\").replace(\"_embeddings\", \"_crops\")\n",
    "                chunks = relevant_embeddings_paths[index].split(\"_\")\n",
    "                nn_superpixel_embedding = np.load(relevant_embeddings_paths[index])\n",
    "                nn_full_image_embedding = np.load(chunks[0] + \"_\" + chunks[1] + \"_full_embedding_50.npy\")\n",
    "                nn_full_image_filename = (chunks[0] + \"_\" + chunks[1] + \"_full.jpg\").replace(\"_embeddings\", \"_crops\")\n",
    "\n",
    "                re.sub(r'^[0-9]{12}$', 'full', relevant_embeddings_paths[index])\n",
    "            else:\n",
    "                nn_crop_filename = relevant_embeddings_paths[index].replace(\"_embedding_18.npy\", \".jpg\").replace(\"_embeddings\", \"_crops\")    \n",
    "                chunks = relevant_embeddings_paths[index].split(\"_\")\n",
    "                nn_superpixel_embedding = np.load(relevant_embeddings_paths[index])\n",
    "                nn_full_image_embedding = np.load(chunks[0] + \"_\" + chunks[1] + \"_full_embedding_18.npy\")  \n",
    "                nn_full_image_filename = (chunks[0] + \"_\" + chunks[1] + \"_full.jpg\").replace(\"_embeddings\", \"_crops\")\n",
    "\n",
    "            # add to list of nearest neighbor full image embeddings\n",
    "            nn_full_image_embeddings.append(nn_full_image_embedding)\n",
    "            # add superpixel embeddings to list as well\n",
    "            nn_superpixel_embeddings.append(nn_superpixel_embedding)\n",
    "\n",
    "        negative_nn_embeddings_for_update = []\n",
    "\n",
    "        # compute the corresponding full image embeddings for the update\n",
    "        for i in range(0, len(N_NEAREST_NEIGHBORS)):\n",
    "            negative_nn_embeddings_for_update.append(np.array(nn_full_image_embeddings[:N_NEAREST_NEIGHBORS[i]]))\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        # Next, we randomly draw a POSITIVE from the validation data to inspect and understand why the prediction has been made. \n",
    "\n",
    "        ### We visualize the randomly-selected image and report the predicted class below:\n",
    "\n",
    "        def get_embedding(index):\n",
    "            if RESNET_50_BOOL:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(index) + \"_full_embedding_50.npy\"\n",
    "            else:\n",
    "                filename = \"../COCO/train2017_embeddings/\" + '{:>012d}'.format(index) + \"_full_embedding_18.npy\"\n",
    "            embedding = np.load(filename)\n",
    "            return embedding\n",
    "\n",
    "        # first, let's randomly grab a positive image from VAL\n",
    "        positive_instance_index = random.choice(positive_update_ids)\n",
    "\n",
    "        positive_img = get_image(positive_instance_index)\n",
    "        positive_instance_embedding = get_embedding(positive_instance_index)\n",
    "\n",
    "        prediction = clf.predict(np.array([positive_instance_embedding]))\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        ### Next, we run LIME:\n",
    "\n",
    "        # need this transform defined in https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb\n",
    "        def get_pil_transform(): \n",
    "            transf = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.CenterCrop(224)\n",
    "            ])    \n",
    "\n",
    "            return transf \n",
    "\n",
    "        pill_transf = get_pil_transform()\n",
    "\n",
    "        from lime import lime_image\n",
    "\n",
    "        explainer = ModifiedLimeImageExplainer()\n",
    "        explanation = explainer.explain_instance(np.array(pill_transf(positive_img)),\n",
    "                                                 positive_instance_embedding, # pass in embedding here\n",
    "                                                 clf.predict_proba, # classification function\n",
    "                                                 top_labels=5, \n",
    "                                                 hide_color=0, \n",
    "                                                 batch_size=1,\n",
    "                                                 num_samples=N_LIME_SAMPLES) # number of images that will be sent to classification function\n",
    "\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        ### Next, we show the explanation and retrieve the corresponding crop from the preprocessed data:\n",
    "\n",
    "        # get LIME explanation\n",
    "        temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=1, hide_rest=False)\n",
    "        img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
    "\n",
    "        # we now find the segment mask for the explanation\n",
    "        exp_segment_rows, exp_segment_cols = np.where(mask != 0)\n",
    "\n",
    "        # now, we load the mask from the corresponding image to map back to the crop\n",
    "        segments_filename = \"../COCO/train2017_segments/\" + '{:>012d}'.format(positive_instance_index) + \".npy\"\n",
    "        segments = np.load(segments_filename)\n",
    "\n",
    "        # for each pixel in the explanation mask, we retrieve the ID in the mask and check to see what it corresponds to\n",
    "        segment_ids = []\n",
    "        for i in range(0, len(exp_segment_rows)):\n",
    "            segment_ids.append(segments[exp_segment_rows[i], exp_segment_cols[i]])\n",
    "\n",
    "        # we now check to make sure that this is all one segment\n",
    "        try:\n",
    "            assert len(set(segment_ids)) == 1\n",
    "        except:\n",
    "            print(\"Explanation weirdness - multiple superpixels reported with top explanation ID - maybe strange tie?\")\n",
    "\n",
    "\n",
    "        # if so, we record the segment ID\n",
    "        positive_explanation_segment_id = segment_ids[0]\n",
    "\n",
    "        # we now retrieve the cropped image\n",
    "        crop_filename = \"../COCO/train2017_crops/\" + '{:>012d}'.format(positive_instance_index) + \"_\" + str(positive_explanation_segment_id) + \".jpg\"\n",
    "\n",
    "        catId = coco.getCatIds(catNms=[category_name])[0]\n",
    "\n",
    "        # now, we set all superpixels containing that \"object\" as the superpixels to act on\n",
    "        positive_action_segment_ids = retrieve_relevant_superpixels(positive_instance_index, catId)\n",
    "\n",
    "        # set positive feedback as 1\n",
    "        positive_feedback = 1\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        # Next, we show the superpixel acted on, along with the annotation, and set the embedding:\n",
    "\n",
    "        mask = np.zeros(segments.shape)\n",
    "        for positive_action_segment_id in positive_action_segment_ids:\n",
    "            mask += (segments == positive_action_segment_id)\n",
    "\n",
    "        # now, we generate bounding box around these segments\n",
    "        boxes = find_objects(mask.astype(int))\n",
    "        try:\n",
    "            box = boxes[0]\n",
    "        except:\n",
    "            continue\n",
    "        w1 = box[1].start\n",
    "        w2 = box[1].stop\n",
    "        h1 = box[0].start\n",
    "        h2 = box[0].stop\n",
    "        pil_im = Image.fromarray(np.uint8(np.array(pill_transf(get_image(positive_instance_index))))).convert('RGB')\n",
    "        cropped = pil_im.crop((w1, h1, w2, h2))\n",
    "\n",
    "        # we now feed it to the embedding model\n",
    "        # load in img2vec\n",
    "        # generate appropriate ResNet embedding\n",
    "        if RESNET_50_BOOL:\n",
    "            img2vec_resnet_50 = Img2Vec(cuda=True, model='resnet-50') \n",
    "            crop_embedding = img2vec_resnet_50.get_vec(cropped, tensor=False)\n",
    "        else:\n",
    "            img2vec_resnet_18 = Img2Vec(cuda=True, model='resnet-18') \n",
    "            crop_embedding = img2vec_resnet_18.get_vec(cropped, tensor=False)\n",
    "\n",
    "        positive_superpixel_embedding = crop_embedding\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "\n",
    "        ### Next, we retrieve the most similar crops across all images:\n",
    "\n",
    "        # next, we compute the nearest neighbors using the embeddings \n",
    "        cosine_vectors = relevant_embeddings - crop_embedding\n",
    "        cosine_distances = np.sum(np.abs(cosine_vectors)**2,axis=-1) #**(1./2)\n",
    "        positive_nearest_indices = np.argsort(cosine_distances)[:np.max(np.array(N_NEAREST_NEIGHBORS))]\n",
    "        \n",
    "        positive_instance_cosine_vectors = relevant_embeddings - positive_instance_embedding\n",
    "        positive_instance_cosine_distances = np.sum(np.abs(positive_instance_cosine_vectors)**2,axis=-1)\n",
    "        positive_instance_nearest_indices = np.argsort(positive_instance_cosine_distances)[:np.max(np.array(N_NEAREST_NEIGHBORS))*1000]\n",
    "        positive_instance_nearest_embeddings = []\n",
    "\n",
    "        for i in range(0, len(positive_instance_nearest_indices)):\n",
    "            # we want the full images here!\n",
    "            if \"_full\" in relevant_embeddings_paths[positive_instance_nearest_indices[i]]:\n",
    "                positive_instance_nearest_embeddings.append(np.load(relevant_embeddings_paths[positive_instance_nearest_indices[i]]))\n",
    "                coco_id = int(relevant_embeddings_paths[positive_instance_nearest_indices[i]].split('_')[1].split(\"embeddings/\")[1])\n",
    "                if len(positive_instance_nearest_embeddings) == np.max(np.array(N_NEAREST_NEIGHBORS)):\n",
    "                    break\n",
    "        positive_instance_nearest_embeddings = np.array(positive_instance_nearest_embeddings)\n",
    "\n",
    "        # print(\"Done identifying nearest neighbors....\")\n",
    "\n",
    "        nn_full_image_embeddings = []\n",
    "        nn_superpixel_embeddings = []\n",
    "\n",
    "        # we now show the nearest crops and grab the relevant full image embeddings\n",
    "        for i in range(0, max_neighbors):\n",
    "\n",
    "            index = positive_nearest_indices[i] # if scikit-learn, use [0][i] here\n",
    "            if RESNET_50_BOOL:\n",
    "                nn_crop_filename = relevant_embeddings_paths[index].replace(\"_embedding_50.npy\", \".jpg\").replace(\"_embeddings\", \"_crops\")\n",
    "                nn_superpixel_embedding = np.load(relevant_embeddings_paths[index])\n",
    "                chunks = relevant_embeddings_paths[index].split(\"_\")\n",
    "                nn_full_image_embedding = np.load(chunks[0] + \"_\" + chunks[1] + \"_full_embedding_50.npy\")\n",
    "                nn_full_image_filename = (chunks[0] + \"_\" + chunks[1] + \"_full.jpg\").replace(\"_embeddings\", \"_crops\")\n",
    "\n",
    "                re.sub(r'^[0-9]{12}$', 'full', relevant_embeddings_paths[index])\n",
    "            else:\n",
    "                nn_crop_filename = relevant_embeddings_paths[index].replace(\"_embedding_18.npy\", \".jpg\").replace(\"_embeddings\", \"_crops\")    \n",
    "                nn_superpixel_embedding = np.load(relevant_embeddings_paths[index])\n",
    "                chunks = relevant_embeddings_paths[index].split(\"_\")\n",
    "                nn_full_image_embedding = np.load(chunks[0] + \"_\" + chunks[1] + \"_full_embedding_18.npy\")  \n",
    "                nn_full_image_filename = (chunks[0] + \"_\" + chunks[1] + \"_full.jpg\").replace(\"_embeddings\", \"_crops\")\n",
    "\n",
    "            # add to list of nearest neighbor full image embeddings\n",
    "            nn_full_image_embeddings.append(nn_full_image_embedding)\n",
    "            # add superpixel embeddings as well\n",
    "            nn_superpixel_embeddings.append(nn_superpixel_embedding)\n",
    "\n",
    "        positive_nn_embeddings_for_update = []\n",
    "        # compute the corresponding full image embeddings for the update\n",
    "        for i in range(0, len(N_NEAREST_NEIGHBORS)):\n",
    "            positive_nn_embeddings_for_update.append(np.array(nn_full_image_embeddings[:N_NEAREST_NEIGHBORS[i]]))\n",
    "            \n",
    "        ####################################################################################################################################\n",
    "\n",
    "        # Now, we update the model and re-train based on the user feedback:\n",
    "\n",
    "        # get gold label for instance\n",
    "        negative_label = 0\n",
    "        if negative_instance_index in filtered_img_ids:\n",
    "            negative_label = 1\n",
    "        positive_label = 0\n",
    "        if positive_instance_index in filtered_img_ids:\n",
    "            positive_label = 1\n",
    "\n",
    "        negative_img = get_image(negative_instance_index)\n",
    "        negative_instance_embedding = get_embedding(negative_instance_index)\n",
    "\n",
    "        # now, we make sure the label and feedback are correct based on FP / FN designation\n",
    "        assert negative_label == 0\n",
    "        assert positive_label == 1\n",
    "        assert negative_feedback == 0\n",
    "        assert positive_feedback == 1\n",
    "        \n",

    "        new_val_scores_neighbors = []\n",
    "        new_test_scores_neighbors = []\n",

    "\n",
    "        # sets the number of nearest neighbors to consider\n",
    "        for i in range(0, len(N_NEAREST_NEIGHBORS)):\n",
    "            # set weight of neighbors relative to other examples in sample\n",
    "            for NEIGHBORS_WEIGHT in NEIGHBORS_WEIGHTS:\n",
    "\n",

    "                ### NOW, WE HANDLE NEAREST NEIGHBORS ###\n",
    "\n",
    "                # instantiate linear model and train\n",
    "                clf_additional_labels = np.zeros(N_NEAREST_NEIGHBORS[i]*2)\n",
    "                clf_additional_labels[:N_NEAREST_NEIGHBORS[i]] += negative_feedback\n",
    "                clf_additional_labels[N_NEAREST_NEIGHBORS[i]:] += positive_feedback\n",
    "\n",
    "                clf = LogisticRegression(random_state=0)\n",
    "                clf.fit(np.concatenate((train_X, np.concatenate((negative_nn_embeddings_for_update[i], positive_nn_embeddings_for_update[i]), axis=0)), axis=0), \n",
    "                        np.concatenate((train_y, clf_additional_labels), axis=0),\n",
    "                        np.concatenate((train_sample_weight, np.zeros(N_NEAREST_NEIGHBORS[i]*2) + NEIGHBORS_WEIGHT/float(N_NEAREST_NEIGHBORS[i])), axis=0)\n",
    "                        )\n",
    "                # evaluate linear model's performance\n",
    "                new_val_score = clf.score(val_X, val_y)\n",
    "                new_val_scores_neighbors.append(new_val_score)\n",
    "                new_test_score = clf.score(test_X, test_y)\n",
    "                new_test_scores_neighbors.append(new_test_score)\n",
    "\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        # Next, we evaluate the change in performance with adding the example to the training data for comparison:\n",
    "\n",
    "        # instantiate linear model and train\n",
    "        clf = LogisticRegression(random_state=0)\n",
    "\n",
    "        clf.fit(np.concatenate((train_X, np.array([get_embedding(negative_instance_index), get_embedding(positive_instance_index)])), axis=0), \n",
    "                np.concatenate((train_y, np.array([negative_label, positive_label]))),\n",
    "                np.concatenate((train_sample_weight, np.array([1., 1.])))\n",
    "                )\n",
    "\n",
    "        # evaluate linear model's performance\n",
    "        new_val_score_added_instance = clf.score(val_X, val_y)\n",
    "        new_test_score_added_instance = clf.score(test_X, test_y)\n",
    "\n",
    "        ####################################################################################################################################\n",
    "\n",
    "        # Lastly, we save the experiment output:\n",
    "\n",
    "        # lastly, let us write the info to an out file\n",
    "\n",
    "        metadata = {}\n",
    "        metadata[\"RESNET_50_BOOL\"] = RESNET_50_BOOL # ResNet-50 or ResNet-18?\n",
    "        metadata[\"TRAIN_SIZE\"] = TRAIN_SIZE # size of training data\n",
    "        metadata[\"N_NEAREST_NEIGHBORS\"] = N_NEAREST_NEIGHBORS # number of nearest neighbors to include\n",
    "        metadata[\"NEIGHBORS_WEIGHTS\"] = NEIGHBORS_WEIGHTS # weights of neighbors for updates (of various strengths)\n",
    "        metadata[\"category_name\"] = category_name # category name (i.e., \"person\")\n",
    "        metadata[\"positive_train_ids\"] = positive_train_ids # IDs of + examples in training data\n",
    "        metadata[\"negative_train_ids\"] = negative_train_ids # IDs of - examples in training data\n",
    "        metadata[\"negative_instance_index\"] = negative_instance_index # ID of FP instance being explained (a misclasification from val data)\n",
    "        metadata[\"positive_instance_index\"] = positive_instance_index # ID of FN instance being explained (a misclasification from val data)\n",
    "        metadata[\"negative_explanation_segment_id\"] = negative_explanation_segment_id # ID of the superpixel in FP explanation\n",
    "        metadata[\"positive_explanation_segment_id\"] = positive_explanation_segment_id # ID of the superpixel in FN explanation\n",
    "        metadata[\"negative_action_segment_id\"] = negative_action_segment_ids # IDs of the FP superpixel acted on\n",
    "        metadata[\"positive_action_segment_id\"] = positive_action_segment_ids # IDs of the FN superpixel acted on\n",
    "        metadata[\"negative_nearest_indices\"] = list(negative_nearest_indices) # IDs of nearest neighbors for FP (indexed into relevant_embeddings_paths) # slice [0] here if using scikit-learn nearest neighbors\n",
    "        metadata[\"positive_nearest_indices\"] = list(positive_nearest_indices) # IDs of nearest neighbors for FN (indexed into relevant_embeddings_paths) # slice [0] here if using scikit-learn nearest neighbors\n",
    "        \n",
    "        metadata[\"original_val_score\"] = original_val_score # original performance of classifier on test set\n",
    "        metadata[\"new_val_score_added_instance\"] = new_val_score_added_instance # new performance of classifier on test set after update with corrected example appended to training data\n",
    "        metadata[\"new_val_scores_neighbors\"] = new_val_scores_neighbors # test scores for nearest neighbors update\n",
    "        \n",
    "        metadata[\"original_test_score\"] = original_test_score # original performance of classifier on test set\n",
    "        metadata[\"new_test_score_added_instance\"] = new_test_score_added_instance # new performance of classifier on test set after update with corrected example appended to training data\n",
    "        metadata[\"new_test_scores_neighbors\"] = new_test_scores_neighbors # test scores for nearest neighbors update\n",
    "\n",
    "        # https://stackoverflow.com/questions/11942364/typeerror-integer-is-not-json-serializable-when-serializing-json-in-python\n",
    "        # need to handle numpy.int64 conversion to make dict JSON serializable\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.int64): return int(o)  \n",
    "            if isinstance(o, np.float32): return float(o)  \n",
    "\n",
    "        # write to outfile with timestamp (bijection, so actual experiment time can be recovered easily)\n",
    "        timestamp = round(time.time() * 1000)\n",
    "        print(\"Saving to: \" + str(timestamp) + \".json...\")\n",
    "        if not os.path.isdir('results/' + str(category_name)):\n",
    "            os.mkdir('results/' + str(category_name))\n",
    "        with open('results/' + str(category_name) + '/' + str(timestamp) + '.json', 'w') as f:\n",
    "            json.dump(metadata, f, default=convert)\n",
    "\n",
    "        ####################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# add precision@10 for recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
